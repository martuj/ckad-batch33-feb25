
CKAD-Cheat-sheet.txt

======================================================================================================================
Lab 1: Bootstrap a Kubernetes Cluster using Kubeadm.

-----------------------------------------------------------------------
Task 1: Launching Instances on AWS
-----------------------------------------------------------------------

#3 VMs

#Instance type t2.medium

#Instead of opening all ports you can open these ports internally.

Nodes	          Port Number	          Use Case
Master, Workers	   2379               etcd Client API
Master, Workers    2380    	   Etcd Server API
Master	           6443	           Kubernetes API Server (Secure Port)
Master, Workers	   6782 – 6784     Weave Net Server/Client API #CNI
Master, Workers	   10250 – 10255   Kubelet Communication
Workers	           30000 – 32767   Reserved of NodePort Ips

-----------------------------------------------------------------------
Task 2: Setting up Machines
-----------------------------------------------------------------------
#All steps in this task are to be performed on all the machines.
#connect all VMs with putty

#Switch to root.
sudo su

 
#install and configure kubeadm using the link on all the 3 instances.


#Create kubeadm.sh script file
vi kubeadm.sh
------------------------------------------------------------------------------------
#!/bin/bash
#
# Common setup for all servers (Control Plane and Nodes)

set -euxo pipefail

# Kuernetes Variable Declaration

KUBERNETES_VERSION="1.29.0-1.1"

# disable swap
sudo swapoff -a

# keeps the swaf off during reboot
(crontab -l 2>/dev/null; echo "@reboot /sbin/swapoff -a") | crontab - || true
sudo apt-get update -y


# Install CRI-O Runtime

OS="xUbuntu_22.04"

VERSION="1.28"

# Create the .conf file to load the modules at bootup
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# sysctl params required by setup, params persist across reboots
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

# Apply sysctl params without reboot
sudo sysctl --system

cat <<EOF | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list
deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/ /
EOF
cat <<EOF | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.list
deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/ /
EOF

curl -L https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:$VERSION/$OS/Release.key | sudo apt-key --keyring /etc/apt/trusted.gpg.d/libcontainers.gpg add -
curl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/Release.key | sudo apt-key --keyring /etc/apt/trusted.gpg.d/libcontainers.gpg add -

sudo apt-get update
sudo apt-get install cri-o cri-o-runc -y

sudo systemctl daemon-reload
sudo systemctl enable crio --now

echo "CRI runtime installed susccessfully"

# Install kubelet, kubectl and Kubeadm

sudo apt-get update -y
sudo apt-get install -y apt-transport-https ca-certificates curl gpg

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-1-28-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-1-28-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes-1.28.list

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-1-29-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-1-29-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes-1.29.list

sudo apt-get update -y
sudo apt-get install -y kubelet="$KUBERNETES_VERSION" kubectl="$KUBERNETES_VERSION" kubeadm="$KUBERNETES_VERSION"
sudo apt-get update -y
sudo apt-mark hold kubelet kubeadm kubectl

sudo apt-get install -y jq

local_ip="$(ip --json addr show eth0 | jq -r '.[0].addr_info[] | select(.family == "inet") | .local')"
cat > /etc/default/kubelet << EOF
KUBELET_EXTRA_ARGS=--node-ip=$local_ip
EOF


--------------------------------------------------------------------------------------
#save the file using "ESCAPE + :wq!"


#Run the script to setup and configure kubeadm on all 3 instances.

. ./kubeadm.sh


-----------------------------------------------------------------------
Task 3: Initializing the Cluster
-----------------------------------------------------------------------
#Set the hostname to all the three nodes as master, worker1, worker2 in their respective terminals for easy understanding, by running the below command:

#on control node(master)
hostnamectl set-hostname master  

#on Worker node-1
hostnamectl set-hostname worker1


#on Worker node-2
hostnamectl set-hostname worker2

#Start kubeadm only on master.
kubeadm init


If it runs successfully, it will provide a join command which can be used to join the master. Make a note of the highlighted part.
---------------------------------------------------------------------------
#If you want to list and generate tokens again to join worker nodes, then follow the below steps.

kubeadm token list
kubeadm token create  --print-join-command
 
----------------------------------------------------------------------- 
Task 4: Joining a Cluster
-----------------------------------------------------------------------

#Run the kubeadm join command in worker nodes, that was previously noted from the
master node in the previous task.

kubeadm join --token <your_token> --discovery-token-ca-cert- hash <your_discovery_token>

 
#Run the following commands to configure kubectl on master.

mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config 
chown $(id -u):$(id -g) $HOME/.kube/config

OR

#Alternatively, if you are the root user, you can run:
export KUBECONFIG=/etc/kubernetes/admin.conf


#View node information on the master. The nodes will not be ready.
kubectl get nodes
     




----------------------------------------------------------------------- 
Task 5: Deploy Container Networking Interface only on master node

----------------------------------------------------------------------- 
# Apply weave CNI (Container Network Interface) as shown below:
kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml

(reference link1:- https://www.weave.works/docs/net/latest/kubernetes/kube-addon/)



#View nodes to see that they are ready.
kubectl get nodes

#View all Pods including system Pods and see that dns and weave are running.
kubectl get pod -n kube-system

----------------------------------------------------------------------- 
Task 6: Create Pods

----------------------------------------------------------------------- 
#Create a Pod called http based on a Docker image on the master.
kubectl run httpd --image=httpd

#View the status of Pod and make sure it’s in the running state.
kubectl get pods


#Get a shell to the container in the Pod using the Pod name from the previous step.
kubectl exec -it <pod_name> -- /bin/bash
i.e
kubectl exec -it httpd -- /bin/bash

   

#Install curl in the container.
apt update
apt install curl -y


#Run curl on the localhost (container) to verify the http installation.
curl localhost 
exit
 
===================================================================================
Lab 2: Jobs and Cronjobs in Kubernetes

-----------------------------------------------------------------------
Task 1: Jobs in kubernetes 
----------------------------------------------------------------------- 
vi job-pod.yaml

apiVersion: batch/v1
kind: Job
metadata:
  name: jobs-hello
spec:
  template:
    metadata:
      name: jobs-pod
    spec:
      containers:
      - name: jobs-ctr
        image: busybox
        args:
        - /bin/sh
        - -c
        - echo HELLO WORLD !!!!!
      restartPolicy: Never

kubectl create -f job-pod.yaml
kubectl get jobs
kubectl get pods

read logs 
kubectl logs jobs-hello-7cfn2 
HELLO WORLD !!!!!

Describe job to see
kubectl describe jobs jobs-hello

kubectl delete -f job.yaml
-----------------------------------------------------------------------
Task 2: Cronjobs 
-----------------------------------------------------------------------
#Create a yaml called cron.yaml. Use the content given below to fill the file

vi cron.yaml
 
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cronjob-hello
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: cronjob-ctr
            image: busybox
            args:
            - /bin/sh
            - -c
            - echo Hello World!
          restartPolicy: OnFailure

kubectl create -f cronjob.yaml

kubectl get cronjobs.batch 
NAME            SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob-hello   */1 * * * *   False     0        28s             42s

kubectl get pod
NAME                           READY   STATUS      RESTARTS   AGE
cronjob-hello-27991419-lz69c   0/1     Completed   0          42s

kubectl logs cronjob-hello-27991419-lz69c 
Hello World!

==============================================================================
==============================================================================

Lab 3: Sidecar container and Patterns

Task: 1 sidecar container
---------------------------
vi sidecar.yaml

apiVersion: v1
kind: Pod
metadata:
  name: pod-sidecar
spec:
  containers:
  - name: app-container
    image: ubuntu:latest
    command: ["/bin/sh"]
    args: ["-c","while true; do date >> /var/log/app.txt; sleep 5; done"]
    volumeMounts:
    - name: share-logs
      mountPath: /var/log/
  - name: sidecar-container
    image: nginx:latest
    ports:
    - containerPort: 80
    volumeMounts:
    - name: share-logs
      mountPath: /usr/share/nginx/html
  volumes:
  - name: share-logs
    emptyDir: {}
	
	
kubectl create -f sidecar.yaml
kubectl get pod

kubectl exec -it pod-sidecar -c sidecar-container -- bash
install curl
apt update && apt install curl -y
 
curl 'http://localhost:80/app.txt'
======================================================================
Self test: complete the init container lab from the below official doc

https://kubernetes.io/docs/concepts/workloads/pods/init-containers/

======================================================================================
==============================================================================
Lab 4  Blue/Green Deployment on Kubernetes 
-------------------------------------------------------------------------------
vi web-blue.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web-blue
      type: web-app
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: web-blue
        type: web-app
    spec:
      containers:
      - image: mandarct/web-blue:v1
        name: web-blue
        ports:
        - containerPort: 80
          protocol: TCP
		 
kubectl create -f web-blue.yaml
kubectl get deploy
kubectl get pods

Now create NodePort service to access application

		 
vi svc-web-lb.yaml

apiVersion: v1
kind: Service
metadata:
  name: web-app-svc-lb
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    type: web-app
  type: NodePort
  ports:
   - port: 80
     targetPort: 80
	 
kubectl create -f svc-web-lb.yaml
kubectl get svc

access you application

deploy following yaml file

vi web-green.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-green
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web-green
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: web-green
        type: web-app
    spec:
      containers:
      - image: mandarct/web-green:v1
        name: web-green
        ports:
        - containerPort: 80
          protocol: TCP

kubectl create -f web-green.yaml
kubectl get deployment
kubectl get pods

access this application using same service that we create previously.

some time will to blue deployment web page, some time green webpage

If you delete the web-green deployment, load-balancer will start sending traffic only to the blue pods


kubectl describe svc web-app-svc-lb

--------------------------------------------------------------------------------
Lab 5: Resource Quotas in Kubernetes
-------------------------------------------------
Task 1: Creating a Namespace
-------------------------------------------------
kubectl create namespace quotas
kubectl get ns
kubectl describe ns quotas
-------------------------------------------------
Task 2: Creating a resourcequota
-------------------------------------------------
vi rq-quotas.yaml

apiVersion: v1
kind: ResourceQuota
metadata:
  name: quota
  namespace: quotas
spec:
  hard:
    requests.cpu: "1"
    requests.memory: 1Gi
    limits.cpu: "2"
    limits.memory: 2Gi

kubectl create -f rq-quotas.yaml
kubectl describe ns quota


------------------------------------------------
Task 3: Verify resourcequota Functionality
-------------------------------------------------
vi rq-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: quota-pod
  namespace: quotas
spec:
  containers:
  - name: quota-ctr
    image: nginx
    resources:
      limits:
        memory: "800Mi"
        cpu: "1000m"
      requests:
        memory: "600Mi"
        cpu: "350m"
    ports:
      - containerPort: 80
	  
kubectl create -f rq-pod.yaml
kubectl describe ns quota
kubectl get quota -n quotas -o yaml

-------------------------------------------------
Task 4: Limiting Number of Pods
-------------------------------------------------
kubectl edit resourcequotas quota -n quotas
add following lines in spec: > hard:
count/deployments.apps: 2
count/replicasets.apps: 3

-------------------------------------------------
Task 5: Clean-up
-------------------------------------------------
#Delete the quota to clean up.
kubectl delete ns quotas
==================================================================================
Lab 6: Liveness and Readiness probes

Task 1: Liveness probes
-------------------------------		  
vi liveness-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  labels:
    app: lns
  name: liveness-pod
spec:
  containers:
  - name: liveness
    image: busybox
    args:
    - /bin/sh
    - -c
    - touch /liveness; sleep 6000
    livenessProbe:
      exec:
        command:
        - cat
        - /liveness
      initialDelaySeconds: 5
      periodSeconds: 5
	  
kubectl get pods	  
kubectl describe pod liveness-exec
 
now login to container and delete following file
kubectl exec -it liveness-pod sh 
# rm -f /liveness
# exit

kubectl get pods
you can see pod is getting restarted

 
Task 2: Readiness probe
------------------------------------------ 

vi readiness.yaml

apiVersion: v1
kind: Pod
metadata:
  labels:
    app: rns
  name: readiness-pod
spec:
  containers:
  - name: readiness
    image: nginx
    args:
    - /bin/bash
    - -c
    - service nginx start; touch /readiness; sleep 6000
    readinessProbe:
      exec:
        command:
        - cat
        - /readiness
      initialDelaySeconds: 5
      periodSeconds: 5
	  
kubectl create -f readiness.yaml
kubectl get pods


login inside pod and delete folloing file 

kubectl exec -it readiness-pod bash 
# rm -f /readiness
# exit


View the Pod events and see that readiness probe has failed
kubectl describe pods readiness-pod

now describe service
kubectl describe svc readiness-svc

End point is gone


Task:3  Startup
-------------------
vi startup-probe.yaml

apiVersion: v1
kind: Pod
metadata:
  name: startup-test
spec:
  containers:
  - name: my-app
    image: busybox
    command: ["sh", "-c", "sleep 40 && httpd -f -p 80"]
    ports:
    - containerPort: 80
    startupProbe:
      httpGet:
        path: /
        port: 80
      initialDelaySeconds: 5
      periodSeconds: 5
      failureThreshold: 6


kubectl apply -f startup-probe.yaml
kubectl get pods
kubectl describe pod startup-test




===================================================================================


Lab 7: ConfigMap

Task 1: Setting container environment variables using configmap
-----------------------------------------------------------------

imperative way to create config-map in environment variables:

kubectl create configmap my-configmap --from-literal mydata=hello_world --dry-run=client -o yaml

vi config-map.yaml

apiVersion: v1
data:
  mydata: hello_world
kind: ConfigMap
metadata:
  name: my-configmap

kubectl create -f config-map.yaml
 kubectl get configmaps


vi cm-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: cm-pod
spec:
  containers:
  - name: quota-ctr
    image: nginx:latest
    ports:
    - containerPort: 80
    env:
    - name: hlo
      valueFrom:
        configMapKeyRef:
           name: my-configmap
           key: mydata
		   
kubectl exec -it cm-pod -- bash
echo $hlo
hello_world
exit

----------------------------------------------------------------

Task 2: Setting configuration file with volume using ConfigMap
----------------------------------------------------------------
vi redis-cm.yaml
 
apiVersion: v1
data:
  redis-config: |
    maxmemory 2mb
    maxmemory-policy allkeys-lru
kind: ConfigMap
metadata:
  name: example-redis-config
  namespace: default

kubectl create -f redis-cm.yaml 
kubectl get cm
kubectl describe cm example-redis-config 

Create pod

vi redis-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: redis
spec:
  containers:
  - name: redis
    image: redis
    env:
    - name: MASTER
      value: "true"
    ports:
    - containerPort: 6379
    volumeMounts:
    - mountPath: /redis-master-data
      name: data
    - mountPath: /redis-master
      name: config
  volumes:
    - name: data
      emptyDir: {}
    - name: config
      configMap:
        name: example-redis-config
        items:
        - key: redis-config
          path: redis.conf

kubectl create -f redis-pod.yaml
read config map's file 
kubectl exec -it redis cat /redis-master/redis.conf
